{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT_J_6B_TriviaQA-v2-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqwNXS-9LwJ"
      },
      "source": [
        "<a href=\"http://colab.research.google.com/github/elsanns/retrieval-demo/blob/main/GPT_J_6B_TriviaQA_v2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook is a demo of running GPT-J on TriviaQA.\n",
        "\n",
        "Adapted from: [GPT_J_6B_TriviaQA.ipynb](https://colab.research.google.com/drive/1lAbbh06PBcx6ykEBRHe0MG10CZBMQ610?usp=sharing)\n",
        "\n",
        "Links:\n",
        "\n",
        "- https://arxiv.org/pdf/1705.03551.pdf\n",
        "- http://nlp.cs.washington.edu/triviaqa/\n",
        "- https://github.com/mandarjoshi90/triviaqa\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIJVqHsh4An"
      },
      "source": [
        "# GPT-J-6B Inference Demo\n",
        "\n",
        "Code from [GPT_J_6B_TriviaQA.ipynb](https://colab.research.google.com/drive/1lAbbh06PBcx6ykEBRHe0MG10CZBMQ610?usp=sharing)\n",
        "\n",
        "<a href=\"http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates how to run the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CMw_dSQKfhT"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\n",
        "\n",
        "!!! **Make sure you are using a TPU runtime!** !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xAFw-LOYfe"
      },
      "source": [
        "!apt install zstd\n",
        "\n",
        "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n",
        "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
        "\n",
        "!time tar -I zstd -xf step_383500_slim.tar.zstd\n",
        "\n",
        "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
        "!pip install -r mesh-transformer-jax/requirements.txt\n",
        "\n",
        "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n",
        "!pip install mesh-transformer-jax/ jax==0.2.12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1UXepF-0Uq"
      },
      "source": [
        "## Setup Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex0qJgaueZtJ"
      },
      "source": [
        "import os\n",
        "import requests \n",
        "from jax.config import config\n",
        "\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
        "requests.post(url)\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIgUVdFLe4A8"
      },
      "source": [
        "Sometimes the next step errors for some reason, just run it again ¯\\\\\\_(ツ)\\_/¯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A5IGYSaeze3"
      },
      "source": [
        "import time\n",
        "\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "\n",
        "from mesh_transformer.checkpoint import read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgKq-X2kmba"
      },
      "source": [
        "params = {\n",
        "  \"layers\": 28,\n",
        "  \"d_model\": 4096,\n",
        "  \"n_heads\": 16,\n",
        "  \"n_vocab\": 50400,\n",
        "  \"norm\": \"layernorm\",\n",
        "  \"pe\": \"rotary\",\n",
        "  \"pe_rotary_dims\": 64,\n",
        "\n",
        "  \"seq\": 2048,\n",
        "  \"cores_per_replica\": 8,\n",
        "  \"per_replica_batch\": 1,\n",
        "}\n",
        "\n",
        "per_replica_batch = params[\"per_replica_batch\"]\n",
        "cores_per_replica = params[\"cores_per_replica\"]\n",
        "seq = params[\"seq\"]\n",
        "\n",
        "\n",
        "params[\"sampler\"] = nucleaus_sample\n",
        "\n",
        "# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\n",
        "params[\"optimizer\"] = optax.scale(0)\n",
        "\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFgRkUgfiNdA"
      },
      "source": [
        "Here we create the network and load the parameters from the downloaded files. Expect this to take around 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwNETD2Uk8nu"
      },
      "source": [
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
        "\n",
        "network = CausalTransformer(params)\n",
        "\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
        "\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-eT7Sw6if4J"
      },
      "source": [
        "## Run Model\n",
        "\n",
        "Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\n",
        "\n",
        "Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).\n",
        "\n",
        "You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\n",
        "\n",
        "*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQ4TTabl4z8"
      },
      "source": [
        "# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVzs2TYlvYeX"
      },
      "source": [
        "def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n",
        "    tokens = tokenizer.encode(context)\n",
        "\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "\n",
        "    start = time.time()\n",
        "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
        "\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "      samples.append(f\"\\033[1m{context}\\033[0m{tokenizer.decode(o)}\")\n",
        "\n",
        "    print(f\"completion done in {time.time() - start:06}s\")\n",
        "    return samples\n",
        "\n",
        "print(infer(\"\"\"Question: What is the capital of Germany?\n",
        "Answer: \"\"\", gen_len=32)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlAK6RbCJYg"
      },
      "source": [
        "top_p = 0.1 \n",
        "temp = 0.1\n",
        "\n",
        "context = \"\"\"Question: What is the capital of Germany?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaFp02qFr2Xq"
      },
      "source": [
        "context = \"\"\"Question: What is the capital of France?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm8VOx6Br4xo"
      },
      "source": [
        "context = \"\"\"Question: Who is the current US president?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9qOJQl8r9oh"
      },
      "source": [
        "context = \"\"\"Question: Who was the US president in 1998?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4CFCd51sDXw"
      },
      "source": [
        "context = \"\"\"Context: Joe Biden is the current US president.\n",
        "Question: Who is the current US president?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=temp, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_ErMxvsP4d"
      },
      "source": [
        "context = \"\"\"Question: Who is the current US president?\n",
        "Background: Joe Biden is the current president of the United States.\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfMYTsNrtb3Z"
      },
      "source": [
        "context = \"\"\"Question: Who is the current US president?\n",
        "Background: Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. \n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEPpskiztkLo"
      },
      "source": [
        "context = \"\"\"Background: Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. \n",
        "Question: Who is the current US president?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcRs83Jttlle"
      },
      "source": [
        "context = \"\"\"Background: Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. Question: Who is the current US president?\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbjJ0Tn9tutr"
      },
      "source": [
        "context = \"\"\"Background: Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. Question: Who is the current US president? Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I2M2xHotyTw"
      },
      "source": [
        "context = \"\"\"Background: Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality. Question: Who is the current US president? Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbuzxoc_ujrQ"
      },
      "source": [
        "context = \"\"\"Question: What is the newest Star Wars movie? Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8NqUwqdvJB4"
      },
      "source": [
        "context = \"\"\"Question: Who has written The Mandalorian? Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5dxFzzuzL3"
      },
      "source": [
        "context = \"\"\"Background: The Mandalorian is an American space Western television series created by Jon Favreau for the streaming service Disney+. It is the first live-action series in the Star Wars franchise, beginning five years after the events of Return of the Jedi (1983). Question: Who has written The Mandalorian? Answer:\"\"\"\n",
        "\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZOccJWAvdhR"
      },
      "source": [
        "context = \"\"\"Question: Who directed the 2020 movie BLACK BOX? Answer:\"\"\"\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QYDm8p9vueo"
      },
      "source": [
        "context = \"\"\"Background: Black Box is a 2020 American horror film directed by Emmanuel Osei-Kuffour Jr. and written by Emmanuel Osei-Kuffour Jr. and Stephen Herman. The film stars Mamoudou Athie, Phylicia Rashad, Amanda Christine, Tosin Morohunfola and Troy James. Jason Blum serves as an executive producer under his Blumhouse Television banner. \n",
        "Question: Who directed the 2020 movie BLACK BOX? \n",
        "Answer:\"\"\"\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srCNcEZLwDFI"
      },
      "source": [
        "context = \"\"\"Question: How many people wrote the 2020 movie BLACK BOX? \n",
        "Answer:\"\"\"\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cxChW6-v7O3"
      },
      "source": [
        "context = \"\"\"Background: Black Box is a 2020 American horror film directed by Emmanuel Osei-Kuffour Jr. and written by Emmanuel Osei-Kuffour Jr. and Stephen Herman. The film stars Mamoudou Athie, Phylicia Rashad, Amanda Christine, Tosin Morohunfola and Troy James. Jason Blum serves as an executive producer under his Blumhouse Television banner. \n",
        "Based on the previous paragraph, what is the answer to \"How many people wrote the 2020 movie BLACK BOX?\" \"\"\"\n",
        "print(infer(top_p=top_p, temp=temp, gen_len=32, context=context)[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-4rKhyU4PZO"
      },
      "source": [
        "# TriviaQA dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MGZ9hWy9ZK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0522bdc-55fa-4cd1-ae8a-036204251cc2"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Download sample data from a temporary repo\n",
        "% cd /\n",
        "! git clone https://github.com/elsanns/retrieval-demo.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "Cloning into 'retrieval-demo'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 18 (delta 6), reused 13 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m9cWjld9dLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab49530d-671f-4d52-c549-428e5ca739a9"
      },
      "source": [
        "data_file = 'retrieval-demo/triviaqa/samples/verified-web-dev.json'\n",
        "with open(data_file) as f:\n",
        "    dataset_json = json.load(f)\n",
        "\n",
        "# print(data_json)\n",
        "print(dataset_json.keys())\n",
        "print(dataset_json['Data'][0].keys())\n",
        "\n",
        "# Subset of samples used in the demo\n",
        "sample_json = [x for x in dataset_json['Data'] if len(x['SearchResults']) == 1]\n",
        "sample_json = [x for x in sample_json if 'MatchedWikiEntityName' in x['Answer'].keys()]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Data', 'Domain', 'Split', 'VerifiedEval', 'Version'])\n",
            "dict_keys(['Answer', 'EntityPages', 'Question', 'QuestionId', 'QuestionPartOfVerifiedEval', 'QuestionSource', 'QuestionVerifiedEvalAttempt', 'SearchResults'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erY8WcGB4Zbg"
      },
      "source": [
        "# Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muc_UxD24WOM"
      },
      "source": [
        "Examples of context data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmvvNt-94a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047ba78e-4b2b-4c01-d3ab-f9f5a8a6e04f"
      },
      "source": [
        "sample_no = 0\n",
        "print(sample_json[sample_no]['Question'])\n",
        "print('---------------------------------------')\n",
        "print(sample_json[sample_no]['SearchResults'][0]['Description'])\n",
        "print(sample_json[sample_no]['SearchResults'][0]['Title'])\n",
        "print(sample_json[sample_no]['SearchResults'][0]['Filename'])\n",
        "# print(sample_json[sample_no]['EntityPages']['FileName'])\n",
        "print('---------------------------------------')\n",
        "sample_json[sample_no]['SearchResults']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rita Coolidge sang the title song for which Bond film?\n",
            "---------------------------------------\n",
            "... Rita Coolidge Performing The title track to the JAMES BOND film OCTOPUSSY. Clip from THE VAL DOONICAN MUSIC SHOW 1983 Featuring Rita Coolidge ... HIGH ...\n",
            "RITA COOLIDGE ALL TIME HIGH James Bond 007 OCTOPUSSY The ...\n",
            "158/158_2486.txt\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Description': '... Rita Coolidge Performing The title track to the JAMES BOND film OCTOPUSSY. Clip from THE VAL DOONICAN MUSIC SHOW 1983 Featuring Rita Coolidge ... HIGH ...',\n",
              "  'DocPartOfVerifiedEval': True,\n",
              "  'DocVerifiedEvalAttempt': True,\n",
              "  'Filename': '158/158_2486.txt',\n",
              "  'HumanAnswer': 'OCTOPUSSY',\n",
              "  'Rank': 0,\n",
              "  'Title': 'RITA COOLIDGE ALL TIME HIGH James Bond 007 OCTOPUSSY The ...',\n",
              "  'Url': 'http://www.youtube.com/watch?v=CQ2rD2ZTCB0'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs37uFVN4b5h"
      },
      "source": [
        "Sample templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "mDh2heRH4hIv",
        "outputId": "37a0ee6a-1a30-40ee-f1fb-8d27f7c7ac20"
      },
      "source": [
        "from jinja2 import Template\n",
        "\n",
        "templates_str = {\n",
        "    'template_1': \"Background: {{ SearchResults[0]['Description'] }} Question: {{ Question }}\",\n",
        "    'template_2': \"Question: {{ Question }}, Evidence: {{ SearchResults[0]['Description'] }}\"\n",
        "}\n",
        "\n",
        "templates = {template_name: Template(template_text) for \\\n",
        "             template_name, template_text in templates_str.items()}\n",
        "\n",
        "inputs_json = sample_json[0:10]\n",
        "\n",
        "inputs = [(template_name, sample['QuestionId'], \n",
        "           templates[template_name].render(sample)) for\n",
        "          template_name in templates for sample in inputs_json]\n",
        "aux = list(zip(*inputs))\n",
        "\n",
        "inputs_df = pd.DataFrame({'template': aux[0],\n",
        "                          'question_id': aux[1],\n",
        "                          'input': aux[2]})\n",
        "\n",
        "# pd.set_option(\"max_colwidth\", 100)\n",
        "inputs_df.sort_values(by=['question_id']).head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>template</th>\n",
              "      <th>question_id</th>\n",
              "      <th>input</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>template_1</td>\n",
              "      <td>tc_1007</td>\n",
              "      <td>Background: ... Kiefer Sutherland, Lou Diamond Phillips, Christian Slater. ... Born Today; Celeb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>template_2</td>\n",
              "      <td>tc_1007</td>\n",
              "      <td>Question: Who was born first, Kiefer Sutherland or Christian Slater?, Evidence: ... Kiefer Suthe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>template_1</td>\n",
              "      <td>tc_1020</td>\n",
              "      <td>Background: When they debuted at the Monterey Pop Festival in 1967, Hendrix set his guitar on fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>template_2</td>\n",
              "      <td>tc_1020</td>\n",
              "      <td>Question: Who set fire to his guitar at the Monterey Pop festival in 19676?, Evidence: When they...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>template_1</td>\n",
              "      <td>tc_1156</td>\n",
              "      <td>Background: Murder on the Orient Express movie YIFY ... Swedish: subtitle Murder on the Orient E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      template  ...                                                                                                input\n",
              "3   template_1  ...  Background: ... Kiefer Sutherland, Lou Diamond Phillips, Christian Slater. ... Born Today; Celeb...\n",
              "13  template_2  ...  Question: Who was born first, Kiefer Sutherland or Christian Slater?, Evidence: ... Kiefer Suthe...\n",
              "4   template_1  ...  Background: When they debuted at the Monterey Pop Festival in 1967, Hendrix set his guitar on fi...\n",
              "14  template_2  ...  Question: Who set fire to his guitar at the Monterey Pop festival in 19676?, Evidence: When they...\n",
              "5   template_1  ...  Background: Murder on the Orient Express movie YIFY ... Swedish: subtitle Murder on the Orient E...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "550baI6x1-Ss"
      },
      "source": [
        "Optionally, save templates to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFWMj92l1_5A"
      },
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "def save_templates(new_templates, file_name):\n",
        "    with open(file_name, 'w+') as f:    \n",
        "        yaml.dump(new_templates, f)  \n",
        "\n",
        "def load_templates(file_name):\n",
        "    if os.path.isfile(file_name):\n",
        "        with open(file_name, 'r') as f:\n",
        "            return yaml.safe_load(f)   \n",
        "    return None \n",
        "\n",
        "def add_templates(new_templates, file_name, replace=True):\n",
        "    existing_templates = load_templates(file_name)\n",
        "    if existing_templates is not None:\n",
        "        common_keys = existing_templates.keys() & new_templates.keys()\n",
        "        if len(common_keys) > 0 and not replace:\n",
        "            raise RuntimeError(\"Conflictiong keys: \",common_keys)    \n",
        "    else:\n",
        "        existing_templates = {} \n",
        "\n",
        "    existing_templates.update(new_templates)\n",
        "    save_templates(existing_templates, file_name)    \n",
        "\n",
        "#file_name = '<FILE_NAME>'\n",
        "#templates_str = load_templates(file_name)\n",
        "#save_templates(templates_str, file_name)\n",
        "#add_templates(templates_str, file_name, replace=True)  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4_CeD1uuCyX"
      },
      "source": [
        "# Run on TriviaQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOFMjx1ouUSF"
      },
      "source": [
        "def infer_triviaqa(context, top_p=0.9, temp=1.0, gen_len=512):\n",
        "    tokens = tokenizer.encode(context)\n",
        "\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "\n",
        "    start = time.time()\n",
        "    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
        "\n",
        "    samples_raw = []\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "      decoded_o = tokenizer.decode(o)\n",
        "      samples_raw.append(decoded_o)\n",
        "      samples.append(f\"\\033[1m{context}\\033[0m{decoded_o}\")\n",
        "\n",
        "    print(f\"completion done in {time.time() - start:06}s\")\n",
        "    return samples_raw[0], samples[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZYmAXbv60y9"
      },
      "source": [
        "top_p = 0.1 \n",
        "temp = 0.1\n",
        "gen_length = 32\n",
        "\n",
        "outputs = [(template, question_id, input, \\\n",
        "            infer_triviaqa(input, top_p=top_p, temp=temp, \\\n",
        "                           gen_len=gen_length)[0]) for template, question_id, input in inputs]\n",
        "\n",
        "aux = list(zip(*outputs))\n",
        "\n",
        "outputs_df = pd.DataFrame({'template': aux[0],\n",
        "                           'question_id': aux[1],\n",
        "                           'input': aux[2],\n",
        "                           'output': aux[3]})\n",
        "outputs_df.sort_values(by=['question_id']).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-rLgPCuGox"
      },
      "source": [
        "# Evaluate on TriviaQA\n",
        "\n",
        "Adapted from:\n",
        "https://github.com/mandarjoshi90/triviaqa/blob/master/evaluation/triviaqa_evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUXGkLjHlrX"
      },
      "source": [
        "% cd /\n",
        "! git clone https://github.com/mandarjoshi90/triviaqa.git\n",
        "% cd triviaqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACpB1vlWekXo"
      },
      "source": [
        "from evaluation.triviaqa_evaluation import (\n",
        "    get_ground_truths,\n",
        "    metric_max_over_ground_truths,\n",
        "    exact_match_score,\n",
        "    f1_score)\n",
        "\n",
        "\n",
        "def get_answer_for_qid(dataset_json, qid):\n",
        "    return [x['Answer'] for x in dataset_json['Data'] if x['QuestionId']==qid][0]\n",
        "\n",
        "\n",
        "def get_ground_truths_for_qid(dataset_json, qid):\n",
        "    answer = get_answer_for_qid(dataset_json, qid)\n",
        "    return get_ground_truths(answer)\n",
        "\n",
        "\n",
        "def contains_answer_score(prediction, ground_truth):\n",
        "    return ground_truth.lower() in prediction.lower()\n",
        "\n",
        "\n",
        "def get_ground_truths_all(dataset_json, predictions):\n",
        "    return {qid: get_ground_truths_for_qid(dataset_json, qid) \\\n",
        "            for qid in predictions.keys()}\n",
        "\n",
        "\n",
        "template_names = set(x[0] for x in outputs)\n",
        "outputs_eval_json = {}\n",
        "\n",
        "for template in template_names:\n",
        "    outputs_eval_json[template] = {}\n",
        "    for x in outputs:\n",
        "        if x[0] == template:\n",
        "            outputs_eval_json[template][x[1]] = x[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPiRFkr8hJLD"
      },
      "source": [
        "scores = []\n",
        "\n",
        "for template, predictions in outputs_eval_json.items():\n",
        "    ground_truths_all = get_ground_truths_all(dataset_json, predictions)\n",
        "\n",
        "    for question_id, prediction in predictions.items():\n",
        "        ground_truths = get_ground_truths_all(dataset_json, predictions)[question_id]\n",
        "        exact_match = metric_max_over_ground_truths(exact_match_score, \n",
        "                                                    prediction, \n",
        "                                                    ground_truths)\n",
        "        f1 = metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
        "        contains_answer = metric_max_over_ground_truths(contains_answer_score, \n",
        "                                                        prediction, \n",
        "                                                        ground_truths)\n",
        "        scores.append((template, question_id, exact_match, f1, contains_answer))\n",
        "\n",
        "df_cols = {col_name: vals for col_name, vals in \\\n",
        "           zip(['template', 'question_id', 'exact_match', 'f1', 'contains_answer'], \n",
        "               list(zip(*scores)))}\n",
        "df_scores = pd.DataFrame(df_cols)\n",
        "\n",
        "df_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JeYgfxL0NIA"
      },
      "source": [
        "% cd /"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}